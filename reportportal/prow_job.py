# https://googleapis.dev/python/storage/latest/index.html

import glob
import json
import os
import re
import shutil
from datetime import datetime
from urllib import request
from urllib.parse import urlparse

from ruamel.yaml import YAML
from google.cloud import storage
from google.oauth2 import service_account
import pprint


class JobManager:
    """JobManager reads the job configuration from openshift/release to jobs

    An example of job configuration is https://raw.githubusercontent.com/openshift/release/master/ci-operator/config/openshift/openshift-tests-private/openshift-openshift-tests-private-release-4.11__amd64-nightly.yaml
    """

    def __init__(self, url):
        path = urlparse(url).path.split("/")[-1]
        generated_job_name = path.replace(".yaml", "").replace("__", "-")

        # periodic_job_name_partial is part of the job name auto generated by Prow
        # A full job name is determined by the job config file and test name
        self.periodic_job_name_partial = f"periodic-ci-{generated_job_name}"
        with request.urlopen(url) as f:
            content = f.read().decode("utf-8")
            yaml=YAML(typ="safe")
            self.job_config = yaml.load(content)
            pprint.pprint(self.job_config)


class Job:
    """Job fetches the latest build artifacts from a Prow job

    Job can fetch the artifacts by the step name
    """

    # Bucket is the object storage for job's build artifacts
    BUCKET = "origin-ci-test"


    def __init__(self, *, periodic_job_name_partial, config, job_id=None):
        # uncomment if key is json content of the service account
        # service_account_info = json.loads(key)
        # creds = service_account.Credentials.from_service_account_info(service_account_info)

        # uncomment if key is file
        # creds = service_account.Credentials.from_service_account_file(key_file)
        self.storage_client = storage.Client.create_anonymous_client()
        # self.storage_client = storage.client.Client(
        #     project="interop", credentials=creds
        # )

        self.bucket = self.storage_client.bucket(self.BUCKET)
        if "as" in config:
            self.name = f"{periodic_job_name_partial}-{config['as']}"
            self.test_as = config["as"]
        else:
            #periodic-ci-rhpit-interop-tests-master-ocp-412-quay37-quay-e2e-tests-aws-ipi-ocp412/
            self.name = f"{periodic_job_name_partial}-{config['tests'][0]['as']}"
            self.test_as = config['tests'][0]['as']
        
        self.state = "finished"

        # self.steps tell the program from which step it needs to pull artifacts
        self.steps = [ ]

        if job_id:
            self.job_id = job_id
        else:
            blob = self.bucket.get_blob(f"logs/{self.name}/latest-build.txt")
            if not blob:
                raise Exception(
                    f"Failed to get the latest-build.txt from {self.name}, please check if the job configuration has changed"
                )
            with blob.open("rt") as file:
                self.job_id = file.read()
        print(f"Job {self.name} with id [{self.job_id}] will be uploaded.")

        # skip if job is running
        blob = self.bucket.get_blob(
            f"logs/{self.name}/{self.job_id}/finished.json"
        )
        if blob is None:
            self.state = "running"

        self.download_base_dir = '/tmp'
        os.makedirs(self.download_base_dir, exist_ok=True)

        self.artifacts_download_dir = (
            f"/tmp/{self.name}/{self.job_id}/results"
        )
        print(f"Test result will be downloaded to {self.artifacts_download_dir}")
        os.makedirs(self.artifacts_download_dir, exist_ok=True)

    def _get_pre_test_post_steps(self, build_log_text):
        pre = []
        test = []
        post = []

        build_log_lines = str(build_log_text).split('\n')
        total_lines = len(build_log_lines)
        pre_idx = 0
        test_idx = 0
        post_idx = 0
        for i, line in enumerate(build_log_lines):
            if 'Running multi-stage phase pre' in line:
                pre_idx = i
            if 'Running multi-stage phase test' in line or 'Running multi-stage phase XXXX' in line:
                test_idx = i
            if 'Running multi-stage phase post' in line:
                post_idx = i

        for line in build_log_lines[pre_idx:test_idx if test_idx else post_idx]:
            if 'Running step' in line:
                step_name = (line.strip().split(' ')[-1]).replace('.', '')
                pre.append(step_name)

        if test_idx:
            # test step was executed
            for line in build_log_lines[test_idx:post_idx]:
                if 'Running step' in line:
                    step_name = (line.strip().split(' ')[-1]).replace('.', '')
                    test.append(step_name)

        for line in build_log_lines[post_idx:total_lines - 1]:
            if 'Running step' in line:
                step_name = (line.strip().split(' ')[-1]).replace('.', '')
                post.append(step_name)

        return pre, test, post

    def _download_blob(self, blob_path, write_to=None):
        if not write_to:
            write_to = os.path.join(self.download_base_dir, blob_path)

        try:
            # blob = self.bucket.get_blob(blob_path)
            blob = storage.Blob(blob_path, self.bucket)
            if os.path.exists(write_to):
                os.remove(write_to)
            os.makedirs(os.path.dirname(write_to), exist_ok=True)
            with open(write_to, 'wb') as f:
                self.storage_client.download_blob_to_file(blob, f)
        except Exception as e:
            print(f"Download blob failed: {e}")
            return None

        if os.path.exists(write_to):
            return write_to
        return None

    def process_installation_result(self):

        # fetching installation logs
        build_log_blob_path = f"logs/{self.name}/{self.job_id}/build-log.txt"
        print(f"The path of build-log.txt of Prow job: {build_log_blob_path}")
        build_log_file = self._download_blob(build_log_blob_path)

        if not build_log_file:
            print("Could not find build-log.txt, continue")
            return False

        finished_json_blob_path = f"logs/{self.name}/{self.job_id}/finished.json"
        print(f"The path of finished.json of Prow job: {finished_json_blob_path}")
        finished_json_file = self._download_blob(finished_json_blob_path)

        with open(build_log_file) as f:
            build_log_text_job = f.read()

        with open(finished_json_file) as f:
            finished_d = json.load(f)

        pre_steps, test_steps, post_steps = self._get_pre_test_post_steps(build_log_text_job)

        install_succeed = True if pre_steps else False

        test_cases = []

        # install_preparation_phase, e.g request slice quota
        # finished_time = datetime.utcfromtimestamp(finished_d.get('timestamp') if finished_d.get('timestamp') else 0).strftime('%Y-%m-%d %H:%M:%S')
        install_preparation_phase = {
            'error': False,
            'failure': False,
            'name': 'install-preparation-phase',
            'message': 'Install preparation succeeded.',
            'time': "1"
        }
        if not pre_steps:
            install_preparation_phase['error'] = True
            install_preparation_phase['failure'] = True
            install_preparation_phase['message'] = build_log_text_job

        test_cases.append(install_preparation_phase)

        # pre/installation steps
        for step in pre_steps:
            print(f"Step name is {step}")
            # download build-log.txt and finished.json
            short_step_name = str(step).replace("%s-" % self.test_as, "")
            print(f"short_step_name is {short_step_name}")
            step_blob_path = f"logs/{self.name}/{self.job_id}/artifacts/{self.test_as}/{short_step_name}"
            print(f"The path of build-log.txt of step {step} is {step_blob_path}/build-log.txt")
            try:
                build_log = self._download_blob("%s/build-log.txt" % step_blob_path)
                finished = self._download_blob("%s/finished.json" % step_blob_path)
            except Exception as e:
                print(f"Warning, can not find build-log.txt or finished.json of step {step} ...")
                continue

            finished_d = {}
            build_log_text_step = 'Loading build-log content error, please check original logs.'

            try:
                with open(finished) as f:
                    finished_d = json.load(f)
            except Exception as e:
                print(f"Warning, can not load finished.json content step {step}, skip ...")

            try:
                with open(build_log) as f:
                    build_log_text_step = f.read()
            except Exception as e:
                print(f"Warning, can not load build-log.txt content of step {step}, skip ...")

            # finished_time = datetime.utcfromtimestamp(finished_d.get('timestamp') if finished_d.get('timestamp') else 0).strftime('%Y-%m-%d %H:%M:%S')

            # Check result from both finished.json and build log
            #   https://issues.redhat.com/browse/OCPQE-11856
            case_passed = False
            s = "Step %s succeeded" % step # Step e2e-aws-ipi-sdn-p1-ipi-install-monitoringpvc succeeded

            # Sometime finished.json is empty or can't be downloaded (finished_d.get('passed') is None), so check content in build_log only.
            if s in build_log_text_job:
                case_passed = True

            if not case_passed:
                install_succeed = False

            test_case = {
                'error': False if case_passed else True,
                'failure': False if case_passed else True,
                'name': step,
                'message': build_log_text_step,
                'time': "1"
            }

            test_cases.append(test_case)

        #junit_file = f"{self.artifacts_download_dir}/installation_build_log.xml"
        #plain_text_to_junit('Installer', test_cases, junit_file)

        return install_succeed

    def fetch_step_artifacts(self, step):
        """Fetch the artifacts generated by a step"""

        artifacts_dir = f"logs/{self.name}/{self.job_id}/artifacts/{self.test_as}/"
        print(f"Fetching artifacts for job {self.name}...")
        download_dir = self.__download_blobs(
            artifacts_dir, self.artifacts_download_dir, step["artifacts"]
        )
        if not download_dir:
            print("download_dir is none")
            return download_dir

        new_file = os.path.join(download_dir, f"{step['name']}.xml")
        return new_file

    def fetch_test_artifacts(self):
        """Fetch the artifacts generated by a test"""

        artifacts_dir = f"logs/{self.name}/{self.job_id}/artifacts/"
        print(f"Fetching artifacts for job {self.name}...")
        download_dir = self.__download_blobs_xmls(
            artifacts_dir, self.artifacts_download_dir)
        if not download_dir:
            print("download_dir is none")
        return download_dir


    def fetch_job_artifacts(self):
        """Iterate each step and download all artifacts"""
        print(f"Fetching latest artifacts from job {self.name}...")
        for step in self.steps:
            self.fetch_step_artifacts(step)

    def __download_blobs(self, blobs_dir, download_dir, regexp):
        """Download objects from given path that matches the regular expression"""
        blobs = self.storage_client.list_blobs(self.BUCKET, prefix=blobs_dir)

        for blob in blobs:
            blob_name = blob.name.split("/")[-1]
            target = blob_name.replace(blobs_dir, "")
            if re.match(regexp, target):
                download_file = f"{download_dir}/{target}"
                if os.path.exists(download_file):
                    download_file += ".1"
                with open(download_file, "wb") as file:
                    blob.download_to_file(file)
#                 if download_file[-2:] == '.1':
#                     merge_dup_result_files(download_file)

        if len(os.listdir(download_dir)) > 0:
            print(f"Blobs downloaded to {download_dir}")
            return download_dir
        else:
            print("No blobs downloaded, the artifacts may not have been generated")
            return None
    
    def __download_blobs_xmls(self, blobs_dir, download_dir):
        """Download objects from given path that end with xml and contain interop team defined test result string"""
        blobs = self.storage_client.list_blobs(self.BUCKET, prefix=blobs_dir)

        for blob in blobs:
            blob_name = blob.name.split("/")[-1]
            target = blob_name.replace(blobs_dir, "")
            if target.endswith('.xml'):
                #example for quay test poc only,junit_operator.xml contains test result.
                # need define test result naming convention and MODIFY xmls by calling pit tool
                if "operator" in target.lower():                  
                    download_file = f"{download_dir}/{target}"
                    if os.path.exists(download_file):
                        download_file += ".1"
                    with open(download_file, "wb") as file:
                        blob.download_to_file(file)
                        print(f"%s is DOWNLOADED " % target)

        if len(os.listdir(download_dir)) > 0:
            print(f"Blobs downloaded to {download_dir}")
            return download_dir

        else: 
            print("No blobs downloaded, the artifacts may not have been generated")
            return None
